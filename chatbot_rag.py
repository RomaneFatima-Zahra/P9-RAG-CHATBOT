#chatbot_rag.py

"""
Ce script impl√©mente un Chatbot RAG (Retrieval-Augmented Generation) sp√©cialis√© dans 
les recommandations d'√©v√©nements culturels √† Paris.

Fonctionnalit√©s principales :
1. Recherche vectorielle (VectorStore + FAISS) :
   - Index FAISS pour retrouver rapidement les √©v√©nements proches d'une requ√™te.
   - M√©tadonn√©es associ√©es aux vecteurs (titre, description, lieu, dates, mots-cl√©s, etc.).
   - Embeddings g√©n√©r√©s via Embedder/Mistral pour la similarit√© cosinus.
2. Mod√®le de g√©n√©ration (Mistral via LangChain) :
   - G√©n√®re des r√©ponses en langage naturel √† partir du contexte r√©cup√©r√© par FAISS.
   - Respecte un prompt strict pour formater les √©v√©nements de mani√®re claire et lisible.
3. Pipeline RAG (Retrieval-Augmented Generation) :
   - √âtape 1 : r√©cup√©ration des √©v√©nements pertinents depuis VectorStore.
   - √âtape 2 : formatage du contexte pour le LLM.
   - √âtape 3 : g√©n√©ration de la r√©ponse finale via Mistral.
   - Optionnel : stockage des interactions dans `ragas_logs.jsonl` pour l'√©valuation RAGAS.
4. Test manuel interactif :
   - Permet de poser des questions directement dans la console.
   - Affiche la r√©ponse g√©n√©r√©e par le chatbot et enregistre l'interaction.

Structure g√©n√©rale du code :
- ChatbotRAG : classe principale, encapsule VectorStore, LLM et pipeline RAG.
- _setup_prompt : construction du prompt syst√®me pour guider la g√©n√©ration.
- _format_context : transforme les √©v√©nements FAISS en texte lisible pour le LLM.
- _setup_chain : pipeline LangChain pour combiner retrieval + prompt + LLM.
- chat : m√©thode publique pour interroger le chatbot.
- log_for_ragas : journalisation des interactions pour RAGAS.
"""


import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
from pathlib import Path
import os
from typing import List, Dict, Optional
import logging
import json
from dotenv import load_dotenv
from config import EMBEDDING_MODEL, MODEL_NAME 

from langchain_mistralai.chat_models import ChatMistralAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from mistralai import Mistral
from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough
from vectore_store import VectorStore


# --------------------------------------------------------------------
# Logging
# --------------------------------------------------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# --------------------------------------------------------------------
# PATHS
# --------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent

FAISS_INDEX_FILE = BASE_DIR / "data" / "clean_data" / "faiss_index" / "faiss_index.bin"
METADATA_FILE = BASE_DIR / "data" / "clean_data" / "metadata.json"

# --------------------------------------------------------------------
# ENV 
# --------------------------------------------------------------------
load_dotenv()

MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")

if not MISTRAL_API_KEY:
    raise ValueError("‚ö†Ô∏è MISTRAL_API_KEY manquante dans .env")


# --------------------------------------------------------------------
# Class Chatbot RAG
# --------------------------------------------------------------------

class ChatbotRAG: #ChatbotRAG : classe principale, encapsule VectorStore, LLM et pipeline RAG.
    """
    Chatbot RAG bas√© sur le VectorStore + Mistral LLM.
    """

    def __init__(
        self,
        base_dir: Path,
        top_k: int = 1,
        temperature: float = 0.3,
        similarity_threshold: float = 0.35 ):

        # Recherche vectorielle
        logger.info("üì¶ Initialisation du VectorStore...")
        self.vector_store = VectorStore(
            base_dir=base_dir,
            top_k=top_k,
            similarity_threshold= similarity_threshold
        )

        # Initialiser le mod√®le Mistral via LangChain LLM
        logger.info("ü§ñ Initialisation du mod√®le Mistral...")

        self.llm = ChatMistralAI(
            model= MODEL_NAME,
            mistral_api_key= MISTRAL_API_KEY,
            temperature= temperature)

        self.top_k = top_k

         # Cr√©er les prompts
        self._setup_prompt()
        
        # Cr√©er la cha√Æne de traitement LangChain
        self._setup_chain()

        self.last_interaction = None  # Utilis√© pour RAGAS (mais pas expos√©)
        
        logger.info("ü§ñ Chatbot RAG initialis√© et pr√™t.")

# -------------------------------------------------------------------------
# Prompt
# -------------------------------------------------------------------------
    
    def _setup_prompt(self): #construction du prompt syst√®me pour guider la g√©n√©ration.
        self.system_prompt= """

Tu es un assistant sp√©cialis√© dans les recommandations d'√©v√©nements culturels √† Paris.

Ton r√¥le est de :

Recommander des √©v√©nements culturels pertinents bas√©s sur la demande de l'utilisateur.
Fournir une r√©ponse courte, naturelle et utile.
Adapter tes recommandations au contexte et aux besoins exprim√©s.
Te baser uniquement sur les √©v√©nements futurs fournis dans le CONTEXTE.
Si aucun √©v√©nement ne correspond, l'expliquer clairement et proposer une alternative r√©aliste.

R√®gles de pr√©sentation :
R√©pondre uniquement en texte.
Ne jamais utiliser de Markdown, HTML, JSON ou tout autre format de code dans le texte de la r√©ponse.
Ne jamais formater la r√©ponse comme un bloc de code.
Ne jamais afficher les caract√®res sp√©ciaux comme **, *, #, -, `, , ou \n et \n\n dans la r√©ponse.
Ne jamais mentionner les mots Markdown, HTML, JSON ou code.
La r√©ponse doit √™tre r√©dig√©e uniquement avec du texte normal.

Structure attendue :
Pr√©senter chaque √©v√©nement comme un paragraphe unique compos√© de phrases compl√®tes.
Chaque √©v√©nement doit inclure obligatoirement : Titre, Lieu, Ville, Adresse, Dates, Description et Mots-cl√©s s'ils existent.
S'il y a plusieurs √©v√©nements, les s√©parer uniquement par une ligne vide r√©elle (une ligne enti√®rement vide g√©n√©r√©e avec Entr√©e).
Ne jamais encoder les retours √† la ligne.
Ne jamais utiliser de listes, de titres, d‚Äôemoji ou d‚Äô√©num√©rations num√©rot√©es.

Rappel important :
Ne jamais faire r√©f√©rence √† la similarit√©, aux calculs, √† l'algorithme, au syst√®me ou √† une logique technique.

CONTEXTE (r√©sultats de recherche) : {context}        

        """

        self.prompt = ChatPromptTemplate.from_messages(
            [
                ("system", self.system_prompt),
                MessagesPlaceholder(variable_name="history"),
                ("user", "{question}"),
            ]
        )

# -------------------------------------------------------------------------
# Formatage du contexte pour le LLM
# -------------------------------------------------------------------------
    def _format_context(self, events: List[Dict]) -> str: #transforme les √©v√©nements FAISS en texte lisible pour le LLM.
        """Formate les √©v√©nements r√©cup√©r√©s en contexte lisible pour le LLM"""
        if not events:
            return "Aucun √©v√©nement correspondant trouv√© dans la base de donn√©es."

        context_parts = []
        
        for i, event in enumerate(events, 1):
            # Extraire les informations essentielles
            title = event.get("title_fr", "Sans titre")
            description = event.get("description_chunk", "")
            city = event.get("location_city", "")
            venue = event.get("location_name", "")
            address = event.get("location_address", "")
            date_begin = event.get("firstdate_begin", "")
            date_end = event.get("firstdate_end", "")
            keywords = event.get("keywords_fr", [])
            
            # Construire le texte de l'√©v√©nement
            event_text = f"""--- √âv√©nement {i} ---
            Titre : {title}
            Lieu : {venue}
            Ville : {city}
            Adresse : {address}
            Date de d√©but : {date_begin}
            Date de fin : {date_end}
            Mots-cl√©s : {', '.join(keywords) if keywords else 'N/A'}
            Description : {description}
            """
            context_parts.append(event_text)
        
        return ".".join(context_parts)

# -------------------------------------------------------------------------
# Cha√Æne RAG
# -------------------------------------------------------------------------
    def _setup_chain(self): #pipeline LangChain pour combiner retrieval + prompt + LLM.
        """
        Pipeline LangChain :
        question ‚ûú recherche FAISS ‚ûú prompt ‚ûú Mistral ‚ûú r√©ponse finale
        """
         # D√©finir les composants de la cha√Æne

         # Retrieval : retourne les √©v√©nements RAW + Formatage : transforme en texte lisible
      
        def retrieve_and_format(inputs: Dict) -> str:
            """R√©cup√®re les √©v√©nements pertinents via FAISS"""
            query = inputs["question"]
            events = self.vector_store.search(query)

            # ‚¨áÔ∏è On garde le RAW context pour RAGAS
            return {
                "formatted_context": self._format_context(events),
                "raw_context": events,
                "question": inputs["question"],
                "history": inputs.get("history", [])
        }

        retriever = RunnableLambda(retrieve_and_format)

        # √âtape 2 : Construction du prompt
        build_prompt = RunnableLambda(
            lambda x: {
                "raw_context": x["raw_context"],
                "_prompt": self.prompt.invoke({
                "context": x["formatted_context"],
                "question": x["question"],
                "history": x["history"]
            })
        })

        # √âtape 3 : Appel LLM
        call_llm = RunnableParallel({
            "answer": lambda x: self.llm.invoke(x["_prompt"]),
            "raw_context": lambda x: x["raw_context"]
            })
        
        # Cha√Æne compl√®te RAG
        self.rag_chain = retriever | build_prompt | call_llm

        self.output_parser = StrOutputParser()
        logger.info("‚õìÔ∏è Cha√Æne RAG configur√©e")

# -------------------------------------------------------------------------
# Entr√©e utilisateur ‚Üí R√©ponse finale
# ------------------------------------------------------------------------
    
    def chat(self, user_query: str,history: List[Dict] = None ) -> str: #m√©thode publique pour interroger le chatbot
        """
        Point d'entr√©e principal pour interagir avec le chatbot.
        
        Args:
            user_query: Question ou demande de l'utilisateur
            
        Returns:
            R√©ponse g√©n√©r√©e par le chatbot
        """
        logger.info(f"üí¨ Question utilisateur : {user_query}")
        
        try:

            # 1) Appel de la cha√Æne (renvoie dict avant StrOutputParser)
            result = self.rag_chain.invoke({"question": user_query, "history": history or []})
            

            # 2) Extraire le contexte FAISS pour RAGAS
            raw_context = result["raw_context"]

            # 3) R√©ponse textuelle du mod√®le
            answer = self.output_parser.invoke(result["answer"])  # üî• convertit seulement la sortie LLM en texte

            # 4) Stockage interne pour √©valuation ult√©rieure
            self.last_interaction = {
                "question": user_query,
                "answer": answer,
                "contexts": raw_context
                }
            logger.info("‚úÖ R√©ponse g√©n√©r√©e avec succ√®s")

            self.log_for_ragas()
            logger.info("üìù Interaction enregistr√©e pour RAGAS")

            # 5) R√©ponse renvoy√©e √† l‚ÄôAPI
            return answer
        
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration de r√©ponse : {e}")
            return f"D√©sol√©, une erreur s'est produite : {str(e)}"

 # -------------------------------------------------------------------------
    # Logging optionnel pour RAGAS
# -------------------------------------------------------------------------
    def log_for_ragas(self, filepath="ragas_logs.jsonl"): #journalisation des interactions pour RAGAS.
        if not self.last_interaction:
            return
        with open(filepath, "a", encoding="utf-8") as f:
            f.write(json.dumps(self.last_interaction, ensure_ascii=False) + "\n")
        logger.info("üìù Interaction enregistr√©e pour RAGAS.")


# -----------------------------------------------------------------------------
# Test manuel
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    base = Path(__file__).parent.resolve()
    bot = ChatbotRAG(base)

    while True:
        q = input("\n‚ùì Pose-moi une question (ou 'quit') : ")
        if q.lower() in {"quit", "exit"}:
            break
        answer = bot.chat(q)
        bot.log_for_ragas()
        print("\nü§ñ R√©ponse :\n")
        print(answer)